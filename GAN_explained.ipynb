{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2lr7wzhxfgK"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (952532253.py, line 183)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 183\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfake_loss = cross_entropy_signal(fake_labels, ftvrdého štítku 1.0 (reálný), používá LABEL_SMOOTHING_REAL (nastaveno na 0.9). Tím se diskriminátor povzbudí k méně jistým předpovědím pro reálná data, což může pomoci stabilizovat trénink.\u001b[39m\n                                                  ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, optimizers\n",
        "import math\n",
        "from lib.loader import SingleFileExtractor, FolderExtractor\n",
        "import joblib\n",
        "\n",
        "# ==== Parametry ====\n",
        "HDF_PATH = \"/data\"\n",
        "SIGNAL_NAME = \"art\"\n",
        "WINDOW_SIZE = 500\n",
        "BATCH_SIZE = 64\n",
        "LATENT_DIM = 100\n",
        "EPOCHS = 100\n",
        "MODEL_DIR_SIGNAL_GAN = \"models_signal_gan\"\n",
        "os.makedirs(MODEL_DIR_SIGNAL_GAN, exist_ok=True)\n",
        "\n",
        "# Techniky pro stabilizaci\n",
        "LABEL_SMOOTHING_REAL = 0.9 \n",
        "LABEL_SMOOTHING_FAKE = 0.1\n",
        "LEARNING_RATE_G = 2e-4 \n",
        "LEARNING_RATE_D = 1e-4\n",
        "\n",
        "# Pro reprodukovatelnost\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ==== Pomocné Funkce pro Načítání Signálů ====\n",
        "def get_file_paths(folder_path):\n",
        "    try:\n",
        "        folder_extractor = FolderExtractor(folder_path)\n",
        "        return [e._hdf5_file_path for e in folder_extractor._extractors]\n",
        "    except ValueError as e:\n",
        "        print(f\"Chyba při inicializaci FolderExtractor: {e}. Ujistěte se, že '{folder_path}' je složka.\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Neznámá chyba při získávání cest k souborům: {e}\")\n",
        "        return []\n",
        "\n",
        "def load_signal_segments(file_path, annotations_folder_path, signal_name=\"art\"):\n",
        "    extractor = SingleFileExtractor(file_path)\n",
        "    extractor.auto_annotate(annotations_folder_path)\n",
        "    good_segments, _ = extractor.extract(signal_name)\n",
        "    if not good_segments:\n",
        "        return np.array([])\n",
        "    extractor.load_data(good_segments)\n",
        "    concatenated_data = np.concatenate([seg.data for seg in good_segments if seg.data is not None and len(seg.data) > 0])\n",
        "    return concatenated_data\n",
        "\n",
        "def signal_to_windows(signal, window_size, step_size=None):\n",
        "    windows = []\n",
        "    if step_size is None:\n",
        "        step_size = window_size // 4\n",
        "    if len(signal) < window_size:\n",
        "        return np.array(windows)\n",
        "\n",
        "    for i in range(0, len(signal) - window_size + 1, step_size):\n",
        "        window = signal[i:i + window_size]\n",
        "        if not np.isnan(window).any():\n",
        "            windows.append(window)\n",
        "    return np.array(windows)\n",
        "\n",
        "\n",
        "# ==== Načtení a Zpracování Dat ====\n",
        "all_files = get_file_paths(HDF_PATH)\n",
        "all_signal_data_list = []\n",
        "\n",
        "print(\"Zpracovávám soubory a načítám segmenty signálu 'art':\")\n",
        "if not all_files:\n",
        "    print(f\"Ve složce '{HDF_PATH}' nebyly nalezeny žádné HDF5 soubory.\")\n",
        "else:\n",
        "    for file_path in all_files:\n",
        "        base_name = os.path.basename(file_path)\n",
        "        print(f\" - {base_name}\")\n",
        "        try:\n",
        "            signal_data = load_signal_segments(file_path, HDF_PATH, SIGNAL_NAME)\n",
        "            if signal_data.size > WINDOW_SIZE :\n",
        "                 all_signal_data_list.append(signal_data)\n",
        "            elif signal_data.size > 0:\n",
        "                print(f\"   Segment z {base_name} je příliš krátký ({signal_data.size}) pro window_size {WINDOW_SIZE}, bude přeskočen.\")\n",
        "        except Exception as e:\n",
        "            print(f\"   Chyba při zpracování souboru {base_name}: {e}\")\n",
        "\n",
        "\n",
        "if not all_signal_data_list:\n",
        "    print(\"Nebyly nalezeny žádné validní segmenty signálu pro trénování. Program bude ukončen.\")\n",
        "    X_signal_windows_scaled = np.array([])\n",
        "    signal_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "else:\n",
        "    concatenated_total_signal = np.concatenate(all_signal_data_list)\n",
        "    print(f\"Celková délka spojeného signálu: {len(concatenated_total_signal)}\")\n",
        "\n",
        "    if len(concatenated_total_signal) < WINDOW_SIZE:\n",
        "        print(f\"Celková délka spojeného signálu ({len(concatenated_total_signal)}) je menší než WINDOW_SIZE ({WINDOW_SIZE}). Nelze pokračovat.\")\n",
        "        X_signal_windows_scaled = np.array([])\n",
        "        signal_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    else:\n",
        "        signal_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "        scaled_total_signal = signal_scaler.fit_transform(concatenated_total_signal.reshape(-1, 1)).flatten()\n",
        "        joblib.dump(signal_scaler, os.path.join(MODEL_DIR_SIGNAL_GAN, 'signal_scaler.gz'))\n",
        "\n",
        "        X_signal_windows = signal_to_windows(scaled_total_signal, WINDOW_SIZE)\n",
        "\n",
        "        if X_signal_windows.shape[0] == 0:\n",
        "            print(f\"Po vytvoření oken nezůstala žádná data. Zkontrolujte WINDOW_SIZE, délku signálů a step_size.\")\n",
        "            X_signal_windows_scaled = np.array([])\n",
        "        else:\n",
        "            print(f\"Počet získaných signálových oken: {X_signal_windows.shape[0]}, délka okna: {X_signal_windows.shape[1]}\")\n",
        "            X_signal_windows_scaled = X_signal_windows[:, :, np.newaxis].astype(np.float32)\n",
        "\n",
        "\n",
        "if X_signal_windows_scaled.size == 0 :\n",
        "    print(\"Žádná data pro trénování GANu. Přeskakuji trénování a generování.\")\n",
        "else:\n",
        "    train_dataset_signal = tf.data.Dataset.from_tensor_slices(X_signal_windows_scaled).shuffle(X_signal_windows_scaled.shape[0]).batch(BATCH_SIZE)\n",
        "\n",
        "\n",
        "# ==== Architektura GANu pro signály ====\n",
        "def build_generator_signal(latent_dim, output_window_size):\n",
        "  model = tf.keras.Sequential(name=\"Generator_Signal\")\n",
        "  model.add(layers.Input(shape=(latent_dim,)))\n",
        "  initial_reshape_len = math.ceil(output_window_size / 8.0)\n",
        "  num_filters_initial_reshape = 128\n",
        "  initial_dense_units = int(initial_reshape_len * num_filters_initial_reshape)\n",
        "  model.add(layers.Dense(initial_dense_units))\n",
        "  model.add(layers.Reshape((int(initial_reshape_len), num_filters_initial_reshape)))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU(negative_slope=0.2))\n",
        "  model.add(layers.Conv1DTranspose(128, kernel_size=5, strides=2, padding='same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU(negative_slope=0.2))\n",
        "  model.add(layers.Conv1DTranspose(64, kernel_size=5, strides=2, padding='same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU(negative_slope=0.2))\n",
        "  model.add(layers.Conv1DTranspose(32, kernel_size=5, strides=2, padding='same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU(negative_slope=0.2))\n",
        "  model.add(layers.Conv1DTranspose(1, kernel_size=5, strides=1, padding='same', activation='tanh'))\n",
        "  current_length_after_upsample = int(initial_reshape_len * 8)\n",
        "  if current_length_after_upsample != output_window_size:\n",
        "      diff = current_length_after_upsample - output_window_size\n",
        "      if diff < 0:\n",
        "          raise ValueError(f\"Aktuální délka ({current_length_after_upsample}) je menší než cílová ({output_window_size}).\")\n",
        "      crop_start = diff // 2\n",
        "      crop_end = diff - crop_start\n",
        "      model.add(layers.Cropping1D(cropping=(crop_start, crop_end)))\n",
        "  assert model.output_shape == (None, output_window_size, 1), f\"Chybný výstupní tvar generátoru: {model.output_shape}\"\n",
        "  return model\n",
        "\n",
        "def build_discriminator_signal(input_window_size):\n",
        "    model = tf.keras.Sequential(name=\"Discriminator_Signal\")\n",
        "    model.add(layers.Input(shape=(input_window_size, 1)))\n",
        "    model.add(layers.Conv1D(64, kernel_size=5, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU(negative_slope=0.2))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.Conv1D(128, kernel_size=5, strides=2, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(negative_slope=0.2))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.Conv1D(256, kernel_size=5, strides=2, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(negative_slope=0.2))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# ==== Trénování GANu ====\n",
        "cross_entropy_signal = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "def discriminator_loss_signal(real_output, fake_output):\n",
        "    real_labels = tf.ones_like(real_output) * LABEL_SMOOTHING_REAL\n",
        "    real_loss = cross_entropy_signal(real_labels, real_output)\n",
        "\n",
        "    fake_labels = tf.ones_like(fake_output) * LABEL_SMOOTHING_FAKE\n",
        "    fake_loss = cross_entropy_signal(fake_labels, fake_output)\n",
        "\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss_signal(fake_output):\n",
        "  return cross_entropy_signal(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "print(f\"Používám Label Smoothing pro reálné štítky: {LABEL_SMOOTHING_REAL}\")\n",
        "print(f\"Rychlost učení Generátoru: {LEARNING_RATE_G}, Diskriminátoru: {LEARNING_RATE_D}\")\n",
        "\n",
        "generator_optimizer_signal = optimizers.Adam(LEARNING_RATE_G, beta_1=0.5)\n",
        "discriminator_optimizer_signal = optimizers.Adam(LEARNING_RATE_D, beta_1=0.5)\n",
        "\n",
        "generator_signal_model = build_generator_signal(LATENT_DIM, WINDOW_SIZE)\n",
        "discriminator_signal_model = build_discriminator_signal(WINDOW_SIZE)\n",
        "\n",
        "print(\"--- Architektura Generátoru ---\")\n",
        "generator_signal_model.summary()\n",
        "print(\"\\n--- Architektura Diskriminátoru ---\")\n",
        "discriminator_signal_model.summary()\n",
        "\n",
        "@tf.function\n",
        "def train_step_gan_signal(signal_windows_batch):\n",
        "    noise = tf.random.normal([tf.shape(signal_windows_batch)[0], LATENT_DIM])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_signals = generator_signal_model(noise, training=True)\n",
        "\n",
        "        real_output = discriminator_signal_model(signal_windows_batch, training=True)\n",
        "        fake_output = discriminator_signal_model(generated_signals, training=True)\n",
        "\n",
        "        gen_loss = generator_loss_signal(fake_output)\n",
        "        disc_loss = discriminator_loss_signal(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator_signal_model.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator_signal_model.trainable_variables)\n",
        "\n",
        "    generator_optimizer_signal.apply_gradients(zip(gradients_of_generator, generator_signal_model.trainable_variables))\n",
        "    discriminator_optimizer_signal.apply_gradients(zip(gradients_of_discriminator, discriminator_signal_model.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n",
        "\n",
        "def train_gan_signal(dataset, epochs_count):\n",
        "    print(f\"Vstupuji do funkce train_gan_signal, počet epoch: {epochs_count}\")\n",
        "    history_gen_loss = []\n",
        "    history_disc_loss = []\n",
        "\n",
        "    for epoch in range(epochs_count):\n",
        "        print(f\"Začátek epochy {epoch+1}/{epochs_count}.\")\n",
        "        epoch_gen_loss_avg = tf.keras.metrics.Mean()\n",
        "        epoch_disc_loss_avg = tf.keras.metrics.Mean()\n",
        "\n",
        "        batch_idx = 0\n",
        "        for signal_batch in dataset:\n",
        "            if batch_idx == 0 and epoch == 0:\n",
        "                print(f\"   Zpracovávám první dávku ({signal_batch.shape}) v první epoše...\")\n",
        "                print(\"      Volám train_step_gan_signal poprvé...\")\n",
        "\n",
        "            gen_loss, disc_loss = train_step_gan_signal(signal_batch)\n",
        "\n",
        "            if batch_idx == 0 and epoch == 0:\n",
        "                print(f\"      První volání train_step_gan_signal dokončeno. G_loss: {gen_loss:.4f}, D_loss: {disc_loss:.4f}\")\n",
        "\n",
        "            epoch_gen_loss_avg.update_state(gen_loss)\n",
        "            epoch_disc_loss_avg.update_state(disc_loss)\n",
        "            batch_idx += 1\n",
        "\n",
        "        g_loss_val = epoch_gen_loss_avg.result().numpy()\n",
        "        d_loss_val = epoch_disc_loss_avg.result().numpy()\n",
        "        history_gen_loss.append(g_loss_val)\n",
        "        history_disc_loss.append(d_loss_val)\n",
        "\n",
        "        print(f\"Epocha {epoch+1}/{epochs_count} dokončena. Ztráta G: {g_loss_val:.4f}, Ztráta D: {d_loss_val:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % 10 == 0 or epoch == epochs_count - 1 :\n",
        "            generator_signal_model.save(os.path.join(MODEL_DIR_SIGNAL_GAN, f\"gan_generator_signal_epoch_{epoch+1}.keras\"))\n",
        "\n",
        "    return history_gen_loss, history_disc_loss\n",
        "\n",
        "print(\"\\nSpouštím trénování GANu pro signály arteriálního tlaku...\")\n",
        "gen_loss_history_signal, disc_loss_history_signal = train_gan_signal(train_dataset_signal, EPOCHS)\n",
        "\n",
        "\n",
        "print(\"Trénování GANu pro signály dokončeno a modely uloženy.\")\n",
        "\n",
        "\n",
        " # ==== Generování Signálů ====\n",
        "def generate_signals(generator_model, scaler_model, n_signals=5, latent_dim=LATENT_DIM):\n",
        "    noise = tf.random.normal([n_signals, latent_dim])\n",
        "    generated_scaled_signals = generator_model.predict(noise, verbose=0)\n",
        "\n",
        "    generated_signals_unscaled_list = []\n",
        "    for i in range(n_signals):\n",
        "        signal_scaled_one = generated_scaled_signals[i, :, 0]\n",
        "        signal_unscaled_one = scaler_model.inverse_transform(signal_scaled_one.reshape(-1, 1)).flatten()\n",
        "        generated_signals_unscaled_list.append(signal_unscaled_one)\n",
        "\n",
        "    return generated_signals_unscaled_list\n",
        "\n",
        "    try:\n",
        "        loaded_signal_scaler = joblib.load(os.path.join(MODEL_DIR_SIGNAL_GAN, 'signal_scaler.gz'))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Chyba: Soubor se scalerem '{os.path.join(MODEL_DIR_SIGNAL_GAN, 'signal_scaler.gz')}' nebyl nalezen. Používám scaler z trénování.\")\n",
        "        loaded_signal_scaler = signal_scaler\n",
        "\n",
        "    generated_art_signals = generate_signals(generator_signal_model, loaded_signal_scaler, n_signals=3)\n",
        "\n",
        "\n",
        "    # ==== Vizualizace ====\n",
        "    if X_signal_windows_scaled.size > 0 and len(generated_art_signals) > 0:\n",
        "        real_signal_window_scaled_example = next(iter(train_dataset_signal))[0].numpy()\n",
        "        real_signal_window_unscaled_example = loaded_signal_scaler.inverse_transform(real_signal_window_scaled_example[:,0].reshape(-1,1)).flatten()\n",
        "\n",
        "        plt.figure(figsize=(18, 6 * ((len(generated_art_signals) + 1) // 2)))\n",
        "        plt.subplot((len(generated_art_signals) + 1) // 2, 2, 1)\n",
        "        plt.plot(real_signal_window_unscaled_example)\n",
        "        plt.title(f\"Příklad Reálného Signálu 'art' (okno {WINDOW_SIZE} vzorků)\")\n",
        "        plt.xlabel(\"Vzorek\")\n",
        "        plt.ylabel(\"Arteriální tlak\")\n",
        "\n",
        "        for i, gen_signal in enumerate(generated_art_signals):\n",
        "            plt.subplot((len(generated_art_signals) + 1) // 2, 2, i + 2)\n",
        "            plt.plot(gen_signal)\n",
        "            plt.title(f\"Generovaný Signál 'art' {i+1} (okno {WINDOW_SIZE} vzorků)\")\n",
        "            plt.xlabel(\"Vzorek\")\n",
        "            plt.ylabel(\"Arteriální tlak\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    if 'gen_loss_history_signal' in globals() and 'disc_loss_history_signal' in globals():\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(gen_loss_history_signal, label='Ztráta Generátoru (signál)')\n",
        "        plt.plot(disc_loss_history_signal, label='Ztráta Diskriminátoru (signál)')\n",
        "        plt.title(\"Historie Ztrát GANu (signál 'art')\")\n",
        "        plt.xlabel('Epocha')\n",
        "        plt.ylabel('Ztráta')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Historie trénování pro GAN na signálech není k dispozici.\")\n",
        "\n",
        "    output_hdf5_generated_signals_file = os.path.join(MODEL_DIR_SIGNAL_GAN, \"generated_art_signals.hdf5\")\n",
        "    with h5py.File(output_hdf5_generated_signals_file, \"w\") as f_out:\n",
        "        for i, signal_data in enumerate(generated_art_signals):\n",
        "            dataset_name = f\"art_synthetic_{i}\"\n",
        "            f_out.create_dataset(dataset_name, data=signal_data)\n",
        "    print(f\"Vygenerované signály 'art' uloženy do {output_hdf5_generated_signals_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm7j8ZfLehoT"
      },
      "source": [
        "# Načítání knihoven\n",
        "\n",
        "*   **os** - práce se souborovým systémem\n",
        "*   **h5py** - čtení/zápis HDF5 souborů (formát pro ukládání velkých dat)\n",
        "*   **numpy** - práce s poli a numerickými výpočty\n",
        "*   **matplotlib.pyplot** - kreslení grafů\n",
        "*   **sklearn.preprocessing.MinMaxScaler** - škálování dat do zadaného rozsahu\n",
        "*   **tensorflow** - framework pro strojové učení\n",
        "*   **tensorflow.kera**s - API pro tvorbu modelů neuronových sítí\n",
        "*   **math** - základní matematické funkce\n",
        "*   **joblib** - ukládnání a načítání Python objektů\n",
        "*   **loader** - vytvořený skript pro načítání signálů ze souborů\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTVdRAbu8_l1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, optimizers\n",
        "import math\n",
        "from lib.loader import SingleFileExtractor, FolderExtractor\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wzRpXgqhSPE"
      },
      "source": [
        "# Nastavení Parametrů\n",
        "\n",
        "*   **HDF_PATH** - cesta souboru k signálům\n",
        "*   **SIGNAL_NAME** - jméno signálu které chceme generovat (art - arteriální tlak)\n",
        "*   **WIDNOW_SIZE** - délka jednoho segmentu signálu, na kterém bude GAN trénovat\n",
        "*   **BATCH_SIZE** - kolik window_size se zpracuje v jednom tréninkovém kroku\n",
        "*   **LATENT_DIM** - velikost vstupního vektoru náhodného šumu pro generátor\n",
        "*   **EPOCHS** - počet opakování celého tréninkového cyklu nad daty\n",
        "\n",
        "*   **LABEL_SMOOTHING_REAL** - místo toho aby se reálným vzorkům při tréninku dala hodnota 1.0, použije se 0.9 (label smoothing) což pomáhá stabilizovat trénink\n",
        "*   **LABEL_SMOOTHING_FAKE** - to stejné ale pro falešné hodnoty, místo 0.0 se použije 0.1\n",
        "*   **LEARNING_RATE_G** - rychlost učení pro generátor v GANu\n",
        "*   **LEARNING_RATE_D** - rychlost učení pro diskriminátor, je nižžší protože se diskriminátor často učí rychleji než generátor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FipjAejo9IZ6"
      },
      "outputs": [],
      "source": [
        "# ==== Parametry ====\n",
        "HDF_PATH = \"data\"\n",
        "SIGNAL_NAME = \"art\"\n",
        "WINDOW_SIZE = 500\n",
        "BATCH_SIZE = 64\n",
        "LATENT_DIM = 100\n",
        "EPOCHS = 50\n",
        "MODEL_DIR_SIGNAL_GAN = \"models_signal_gan\"\n",
        "os.makedirs(MODEL_DIR_SIGNAL_GAN, exist_ok=True)\n",
        "\n",
        "LABEL_SMOOTHING_REAL = 0.9 \n",
        "LABEL_SMOOTHING_FAKE = 0.1\n",
        "LEARNING_RATE_G = 2e-4 \n",
        "LEARNING_RATE_D = 1e-4\n",
        "\n",
        "# Pro reprodukovatelnost\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f_4fKfgzEEi"
      },
      "source": [
        "# Pomocné funkce\n",
        "\n",
        "*   get_file_paths - jako input to má cestu k souboru signálům a pomocí FolderExtractor třídy z loader knihovny to vrací cesty ke všem dostupným signálům\n",
        "\n",
        "* load_signal_segments - načítá specifické signál segmenty z jednoho HDF5 souboru, používá SingleFileExtractor třídu k zprocesování souboru, autimaticky anotuje data a extractuje (zatím) pouze dobré segmenty, pak je vrací jako jeden numpy array\n",
        "\n",
        "*   signal_to_windows - tato funkce bere jedno dimensionální signál (numpy array), window_size a step_size jako vstup. funkce iteruje skrze signál, extractuje okna a divá se jestli někde nejsou NaN hodnoty. Pouze okna které nemají NaN hodnoty jsou přidány do seznamu. Nakonec se tento seznam vrací jako numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6McFpqyM9Mn8"
      },
      "outputs": [],
      "source": [
        "# ==== Pomocné Funkce pro Načítání Signálů ====\n",
        "def get_file_paths(folder_path):\n",
        "    try:\n",
        "        folder_extractor = FolderExtractor(folder_path)\n",
        "        return [e._hdf5_file_path for e in folder_extractor._extractors]\n",
        "    except ValueError as e:\n",
        "        print(f\"Chyba při inicializaci FolderExtractor: {e}. Ujistěte se, že '{folder_path}' je složka.\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Neznámá chyba při získávání cest k souborům: {e}\")\n",
        "        return []\n",
        "\n",
        "def load_signal_segments(file_path, annotations_folder_path, signal_name=\"art\"):\n",
        "    extractor = SingleFileExtractor(file_path)\n",
        "    extractor.auto_annotate(annotations_folder_path)\n",
        "    good_segments, _ = extractor.extract(signal_name)\n",
        "    if not good_segments:\n",
        "        return np.array([])\n",
        "    extractor.load_data(good_segments)\n",
        "    concatenated_data = np.concatenate([seg.data for seg in good_segments if seg.data is not None and len(seg.data) > 0])\n",
        "    return concatenated_data\n",
        "\n",
        "def signal_to_windows(signal, window_size, step_size=None):\n",
        "    windows = []\n",
        "    if step_size is None:\n",
        "        step_size = window_size // 4\n",
        "    if len(signal) < window_size:\n",
        "        return np.array(windows)\n",
        "\n",
        "    for i in range(0, len(signal) - window_size + 1, step_size):\n",
        "        window = signal[i:i + window_size]\n",
        "        if not np.isnan(window).any():\n",
        "            windows.append(window)\n",
        "    return np.array(windows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-pjuack43ap"
      },
      "source": [
        "# Načtení a zpracování dat\n",
        "\n",
        "Tato skece kódu obstarává načitání, proceosvání a přípravu signálu z HDF5 soborů pro použití při trenování GAN.\n",
        "\n",
        "\n",
        "Jako první se snaží dostat seznam všech cest k souborům signálu ve složce definované v `HDF_PATH` pomocí funkce `get_file_paths` která je výšše\n",
        "\n",
        "Poté iteruje přes každou cestu k souboru a načte signál specifikovaný v `SIGNAL_NAME` pomocí funkce `load_signal_segments` a přidá načtené signály do `all_signal_data_list` seznamu.\n",
        "\n",
        "Poté zřetězí všechny nalezené signály do jednoho pole, vypíše jeho délku a podívá se jestli totální délka je dostatečná k vytvoření alespoň jedno okno s velikostí `WINDOW_SIZE`.\n",
        "\n",
        "Pokud je délka dostatečná tak se inicializuje `MinMaxScaler` který data vyškáluje do rozsahu (-1, 1).\n",
        "\n",
        "To je poté rozděleno do překryvajících se oken s velikostí `WINDOW_SIZE` pomocí `signal_to_window`funkce výše.\n",
        "\n",
        "Jestliže okna byla úspešně vytvořena, vypíše to počet oken a jejich velikost, potom je přetvoří window data k přidání extra dimenzi, aby se to mohlo využít jako vstup pro konvoluční vrstvy v GAN modelch.\n",
        "\n",
        "Nakonec se z signálových oken vytvoří `tf.data.Dataset`. Tento dataset se pak zamíchá pro náhodnost pořadí oken a batchnou do skupin o velikosti `BATCH_SIZE`.\n",
        "\n",
        "Tento připravený dataset (`train_dataset_signal`) bude použit k následujícímu GAN trénování.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd0sHoeI9SWz",
        "outputId": "513c7c04-cb52-4a87-c6ac-02ca1ee35f91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zpracovávám soubory a načítám segmenty signálu 'art':\n",
            " - TBI_021_v2_1_4_18.hdf5\n",
            " - TBI_022_v2_1_6_21.hdf5\n",
            " - TBI_021_v2_1_4_2.hdf5\n",
            " - TBI_022_v2_2_1_15.hdf5\n",
            " - TBI_023_v2_2_2_17.hdf5\n",
            " - TBI_019_v2_1_6_6.hdf5\n",
            " - TBI_024_v2_1_6_5.hdf5\n",
            " - TBI_024_v2_1_3_19.hdf5\n",
            " - TBI_021_v2_1_3_11.hdf5\n",
            " - TBI_024_v2_1_3_0.hdf5\n",
            " - TBI_019_v2_1_6_17.hdf5\n",
            " - TBI_023_v2_1_6_14.hdf5\n",
            " - TBI_019_v2_1_5_10.hdf5\n",
            " - TBI_022_v2_1_7_18.hdf5\n",
            " - TBI_022_v2_1_7_11.hdf5\n",
            " - TBI_022_v2_2_2_6.hdf5\n",
            " - TBI_024_v2_1_5_15.hdf5\n",
            " - TBI_021_v2_2_5_2.hdf5\n",
            " - TBI_021_v2_1_5_15.hdf5\n",
            " - TBI_023_v2_2_2_7.hdf5\n",
            " - TBI_023_v2_2_1_8.hdf5\n",
            " - TBI_023_v2_2_1_6.hdf5\n",
            " - TBI_019_v2_1_4_13.hdf5\n",
            " - TBI_022_v2_2_2_15.hdf5\n",
            " - TBI_023_v2_2_3_3.hdf5\n",
            " - TBI_019_v2_1_5_19.hdf5\n",
            " - TBI_023_v2_1_6_5.hdf5\n",
            " - TBI_023_v2_1_6_1.hdf5\n",
            " - TBI_019_v2_1_4_18.hdf5\n",
            " - TBI_024_v2_1_3_10.hdf5\n",
            " - TBI_021_v2_2_2_17.hdf5\n",
            " - TBI_021_v2_1_3_17.hdf5\n",
            " - TBI_019_v2_1_5_12.hdf5\n",
            " - TBI_019_v2_2_1_0.hdf5\n",
            " - TBI_021_v2_1_5_22.hdf5\n",
            " - TBI_022_v2_1_4_7.hdf5\n",
            " - TBI_016_v2_1_1_12.hdf5\n",
            " - TBI_018_v2_1_3_1.hdf5\n",
            " - TBI_011_v2_2_3_19.hdf5\n",
            " - TBI_013_v2_2_4_3.hdf5\n",
            " - TBI_016_v2_1_5_2.hdf5\n",
            " - TBI_013_v2_1_5_14.hdf5\n",
            " - TBI_011_v2_2_1_4.hdf5\n",
            " - TBI_018_v2_1_5_11.hdf5\n",
            " - TBI_013_v2_2_3_0.hdf5\n",
            " - TBI_017_v2_2_2_8.hdf5\n",
            " - TBI_018_v2_1_3_14.hdf5\n",
            " - TBI_013_v2_1_4_7.hdf5\n",
            " - TBI_018_v2_1_4_15.hdf5\n",
            " - TBI_011_v2_2_4_18.hdf5\n",
            " - TBI_016_v2_1_2_7.hdf5\n",
            " - TBI_016_v2_1_1_13.hdf5\n",
            " - TBI_011_v2_2_2_0.hdf5\n",
            " - TBI_017_v2_1_7_21.hdf5\n",
            " - TBI_016_v2_1_3_13.hdf5\n",
            " - TBI_013_v2_1_3_21.hdf5\n",
            " - TBI_011_v2_2_1_22.hdf5\n",
            " - TBI_011_v2_2_3_23.hdf5\n",
            " - TBI_018_v2_1_5_5.hdf5\n",
            " - TBI_018_v2_1_3_10.hdf5\n",
            " - TBI_013_v2_1_4_8.hdf5\n",
            " - TBI_016_v2_1_5_5.hdf5\n",
            " - TBI_017_v2_2_2_2.hdf5\n",
            " - TBI_003_v2_2_1_6.hdf5\n",
            " - TBI_004b_v2_1_2_2.hdf5\n",
            " - TBI_003_v2_2_1_9.hdf5\n",
            " - TBI_001_v2_3_5_10.hdf5\n",
            " - TBI_004b_v2_1_2_5.hdf5\n",
            " - TBI_002_v4_2_2_2.hdf5\n",
            " - TBI_001_v2_4_3_16.hdf5\n",
            " - TBI_007_v3_1_2_18.hdf5\n",
            " - TBI_005_v2_3_3_16.hdf5\n",
            " - TBI_001_v2_1_2_5.hdf5\n",
            " - TBI_002_v4_2_7_6.hdf5\n",
            " - TBI_005_v2_2_1_12.hdf5\n",
            " - TBI_007_v3_1_2_23.hdf5\n",
            " - TBI_003_v2_2_1_8.hdf5\n",
            " - TBI_002_v4_2_5_20.hdf5\n",
            " - TBI_004b_v2_1_2_4.hdf5\n",
            " - TBI_003_v2_2_1_7.hdf5\n",
            " - TBI_004b_v2_1_3_6.hdf5\n",
            " - TBI_007_v3_1_2_17.hdf5\n",
            " - TBI_002_v4_2_7_8.hdf5\n",
            " - TBI_005_v2_2_2_19.hdf5\n",
            " - TBI_001_v2_1_2_20.hdf5\n",
            " - TBI_004b_v2_1_2_16.hdf5\n",
            " - TBI_003_v2_2_1_5.hdf5\n",
            " - TBI_001_v2_1_3_6.hdf5\n",
            " - TBI_003_v2_2_1_0.hdf5\n",
            " - TBI_002_v4_2_5_5.hdf5\n",
            "Celková délka spojeného signálu: 31143000\n",
            "Počet získaných signálových oken: 249141, délka okna: 500\n"
          ]
        }
      ],
      "source": [
        "# ==== Načtení a Zpracování Dat ====\n",
        "all_files = get_file_paths(HDF_PATH)\n",
        "all_signal_data_list = []\n",
        "\n",
        "print(\"Zpracovávám soubory a načítám segmenty signálu 'art':\")\n",
        "if not all_files:\n",
        "    print(f\"Ve složce '{HDF_PATH}' nebyly nalezeny žádné HDF5 soubory.\")\n",
        "else:\n",
        "    for file_path in all_files:\n",
        "        base_name = os.path.basename(file_path)\n",
        "        print(f\" - {base_name}\")\n",
        "        try:\n",
        "            signal_data = load_signal_segments(file_path, HDF_PATH, SIGNAL_NAME)\n",
        "            if signal_data.size > WINDOW_SIZE :\n",
        "                 all_signal_data_list.append(signal_data)\n",
        "            elif signal_data.size > 0:\n",
        "                print(f\"   Segment z {base_name} je příliš krátký ({signal_data.size}) pro window_size {WINDOW_SIZE}, bude přeskočen.\")\n",
        "        except Exception as e:\n",
        "            print(f\"   Chyba při zpracování souboru {base_name}: {e}\")\n",
        "\n",
        "\n",
        "if not all_signal_data_list:\n",
        "    print(\"Nebyly nalezeny žádné validní segmenty signálu pro trénování. Program bude ukončen.\")\n",
        "    X_signal_windows_scaled = np.array([])\n",
        "    signal_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "else:\n",
        "    concatenated_total_signal = np.concatenate(all_signal_data_list)\n",
        "    print(f\"Celková délka spojeného signálu: {len(concatenated_total_signal)}\")\n",
        "\n",
        "    if len(concatenated_total_signal) < WINDOW_SIZE:\n",
        "        print(f\"Celková délka spojeného signálu ({len(concatenated_total_signal)}) je menší než WINDOW_SIZE ({WINDOW_SIZE}). Nelze pokračovat.\")\n",
        "        X_signal_windows_scaled = np.array([])\n",
        "        signal_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    else:\n",
        "        signal_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "        scaled_total_signal = signal_scaler.fit_transform(concatenated_total_signal.reshape(-1, 1)).flatten()\n",
        "        joblib.dump(signal_scaler, os.path.join(MODEL_DIR_SIGNAL_GAN, 'signal_scaler.gz'))\n",
        "\n",
        "        X_signal_windows = signal_to_windows(scaled_total_signal, WINDOW_SIZE)\n",
        "\n",
        "        if X_signal_windows.shape[0] == 0:\n",
        "            print(f\"Po vytvoření oken nezůstala žádná data. Zkontrolujte WINDOW_SIZE, délku signálů a step_size.\")\n",
        "            X_signal_windows_scaled = np.array([])\n",
        "        else:\n",
        "            print(f\"Počet získaných signálových oken: {X_signal_windows.shape[0]}, délka okna: {X_signal_windows.shape[1]}\")\n",
        "            X_signal_windows_scaled = X_signal_windows[:, :, np.newaxis].astype(np.float32)\n",
        "\n",
        "\n",
        "if X_signal_windows_scaled.size == 0 :\n",
        "    print(\"Žádná data pro trénování GANu. Přeskakuji trénování a generování.\")\n",
        "else:\n",
        "    train_dataset_signal = tf.data.Dataset.from_tensor_slices(X_signal_windows_scaled).shuffle(X_signal_windows_scaled.shape[0]).batch(BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF_nkzz8PyT0"
      },
      "source": [
        "# Architektura GANu pro signály\n",
        "\n",
        "- tato sekce definuje architekturu Generative Adversarial Network (GAN)  specificky definovanou pro generaci signálu. GAN obashuje dva hlavní díly: **Generátor** a **Diskriminátor**\n",
        "\n",
        "\n",
        "## Generátor (`build_generator_signal`)\n",
        " - role generátoru je generovat syntetické data která se podobají reálným datům. Bere náhodný šum vektor jako vstup a transformuje to do signálu.\n",
        "\n",
        " - vytváří `tf.keras.Sequential` model, který je linerání stoh vrstev\n",
        " -`layers.Input(shape=(latent_dim,))` specifikuje vstupní tvar, který je vektor velikosti `latent_dim`. Tohle je šum vektor.\n",
        " - `layers.Dense` a následující `layers.Reshape` vrsva transformuje vstup šum vektoru do tvaru vhodného pro konvoluční vrstvy. `initial_reshape_len` a `num_filters_initial_reshape`jsou spočítaná na základě požadované `output_window_size` hodnotě která zajistí že se signál převzorkuje správně.\n",
        " - `layers.BatchNormalization` vrstva pomáhá stabilizovat trénovací proces.\n",
        " - `layers.LeakyReLU` je aktivační funkce která zavádí nelinearita\n",
        " - `layers.Conv1DTranspose` (také známý jako dekonvoluční nebo převzorkovací vrstvy) jsou použita pro zvýšení délky signálu. Jsou inverzí konvolučních vrstev.\n",
        " - Generátor používá několik `layers.Conv1DTranspose` vrstev s krokem 2, což efektivně zdvojnásobuje délku signálu při každém kroku\n",
        " - Finální `layers.Conv1DTranspose` vrstva má jediný filter a to `tanh` aktivační funkc, která škáluje výsledné hodnoty do rozsahu -1 až 1, aby to sedělo se škálováním trénovacích dat.\n",
        " - `layers.Cropping1D` vrstva je použita pro upravení délky generovaného signálu pokud ten převzorkovací proces nevratí výsledek v `output_window_size` hodnotě.\n",
        " - `assert` slouží k zajištění tomu že finální výstupní tvar generátoru bude správný: `(None, output_window_size, 1)`, kde `None` reprezentuje batch size, `output_window_size` je délka vygenerovaného signál okna, a `1` je počet kanálů (pro jeden signál).\n",
        "\n",
        "\n",
        "## Diskriminátor (`build_discriminator_signal`)\n",
        "- role diskriminátoru je klasifikovat jestli je daný signál reálný (z tréninkových dat) nebo falešný (vygenerovaný generátorem).\n",
        "\n",
        "- také používá `tf.keras.Sequential` API.\n",
        "- `layers.Input(shape=(input_window_size, 1))` specifikuje vstupní tvar, který je okno signálu o velikosti `input_window_size` s jedním kanálem.\n",
        "- `layers.Conv1D` vrstvy jsou použité k extrakci prvků ze signálu. Tyto vrstvy aplikují konvoluční filtry s časovou dimenzí signálu.\n",
        "- `strides=2` parametr v `Conv1D` vrstvách snižuje délku signálu v každém kroku.\n",
        "- `layers.LeakyReLU` aktivace je použitá po konvolučních vrstvách.\n",
        "- `layers.BatchNormalization` je aplikován pro stabilizaci trénování.\n",
        "- `layers.Dropout` je zahrnuta jako regularizační technika, aby se zabránilo přeplnění.\n",
        "- `layers.Flatten` konvertuje 1D konvoluční prvky map do plochého vektoru.\n",
        "- finální `layers.Dense` má jednu výstupní jednotku a používá `sigmoid` aktivační funkci. To vrátí hodnotu mezi 0 a 1, což představuje pravděpodobnost jestli je signál reálný (blíže k 1) nebo falešný (blíže k 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRwfi0f098pN"
      },
      "outputs": [],
      "source": [
        "# ==== Architektura GANu pro signály ====\n",
        "def build_generator_signal(latent_dim, output_window_size):\n",
        "  model = tf.keras.Sequential(name=\"Generator_Signal\")\n",
        "  model.add(layers.Input(shape=(latent_dim,)))\n",
        "  initial_reshape_len = math.ceil(output_window_size / 8.0)\n",
        "  num_filters_initial_reshape = 128\n",
        "  initial_dense_units = int(initial_reshape_len * num_filters_initial_reshape)\n",
        "  model.add(layers.Dense(initial_dense_units))\n",
        "  model.add(layers.Reshape((int(initial_reshape_len), num_filters_initial_reshape)))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU(negative_slope=0.2))\n",
        "  model.add(layers.Conv1DTranspose(128, kernel_size=5, strides=2, padding='same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU(negative_slope=0.2))\n",
        "  model.add(layers.Conv1DTranspose(64, kernel_size=5, strides=2, padding='same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU(negative_slope=0.2))\n",
        "  model.add(layers.Conv1DTranspose(32, kernel_size=5, strides=2, padding='same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU(negative_slope=0.2))\n",
        "  model.add(layers.Conv1DTranspose(1, kernel_size=5, strides=1, padding='same', activation='tanh'))\n",
        "  current_length_after_upsample = int(initial_reshape_len * 8)\n",
        "  if current_length_after_upsample != output_window_size:\n",
        "      diff = current_length_after_upsample - output_window_size\n",
        "      if diff < 0:\n",
        "          raise ValueError(f\"Aktuální délka ({current_length_after_upsample}) je menší než cílová ({output_window_size}).\")\n",
        "      crop_start = diff // 2\n",
        "      crop_end = diff - crop_start\n",
        "      model.add(layers.Cropping1D(cropping=(crop_start, crop_end)))\n",
        "  assert model.output_shape == (None, output_window_size, 1), f\"Chybný výstupní tvar generátoru: {model.output_shape}\"\n",
        "  return model\n",
        "\n",
        "def build_discriminator_signal(input_window_size):\n",
        "    model = tf.keras.Sequential(name=\"Discriminator_Signal\")\n",
        "    model.add(layers.Input(shape=(input_window_size, 1)))\n",
        "    model.add(layers.Conv1D(64, kernel_size=5, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU(negative_slope=0.2))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.Conv1D(128, kernel_size=5, strides=2, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(negative_slope=0.2))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.Conv1D(256, kernel_size=5, strides=2, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(negative_slope=0.2))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcxLJ3M3-_TU"
      },
      "source": [
        "# Trénování GANu\n",
        "\n",
        "tato sekce kódu se zaměřuje na definování a provedení trénovacího procesu pro GAN\n",
        "\n",
        "## Definice Ztrátových funkcí\n",
        "\n",
        "*  `cross_entropy_signal`: inicializuje binární křížovou entropii, což je běžná ztrátová funkce pro klasifikační úlohy, jako je ta, kterou řeší diskriminátor (rozhodování mezi reálným a falešným)\n",
        "\n",
        "*    `discriminator_loss_signal(real_output, fake_output)`: Tato funkce počítá ztrátu pro **diskriminátor**.\n",
        "  *   Bere `real_ouput`(výstup diskriminátoru pro reálné signály) a `fake_output` (výstup diskriminátoru pro falešné signály)\n",
        "  * Pro **reálné signály** používá **label smoothing**. Místo tvrdého štítku `1.0` (reálný), používá `LABEL_SMOOTHING_REAL` (nastaveno na 0.9). Tím se diskriminátor povzbudí k méně jistým předpovědím pro reálná data, což může pomoci stabilizovat trénink.\n",
        "  * Pro **falešné signály** také používá **label smoothing** s `LABEL_SMOOTHING_FAKE` (nastaven na 0.1).\n",
        "  * Totální ztráta diskriminátoru je součet `real_loss` a `fake_loss`. Tento diskriminátor se snaží tyto ztráty minimalizovat.\n",
        "\n",
        "*   `generator_loss_signal(fake_output)`: Tato funkce počítá ztrátu pro **generátor**.\n",
        "  * Bere `fake_output` (výstup diskriminátoru pro falešné signály vygenerované generátorem).\n",
        "  * Cílem generátoru je oklamat diskriminátor, tj. aby diskriminátor ohodnotil falešné signály za \"reálné\". Proto se generátor snaží maximalizovat ztrátu generátoru na falešných datech, což je ekvivalentní minimalizaci **své vlastní ztráty** tím, že se pokouší přimět diskriminátor k předpovědi `1.0` pro falešná data\n",
        "\n",
        "## Optimalizátory a Modely\n",
        "\n",
        "*   Kód vypisuje použité hodnoty pro label a rychlosti učení pro přehlednost.\n",
        "*   `generator_optimizer_signal` a `discriminator_optimizer_signal`: Inicializují **Adam optimalizátory** pro generátor a diskriminátor. Každý má svou vlastní rychlost učení (`LEARNING_RATE_G` a `LEARNING_RATE_D`). Diskriminátor má obvykle nižžší rychlost učení, protože se učí rychleji než generátor a je důležité udržet rovnováhu v tréninku. Parametr `beta_1=0.5` je běžné nastavené pro GAN trénink.\n",
        "*   `generator_signal_model` a `discriminator_signal_model`: Inicializují instance modelů generátorů a diskriminátorů pomocí dříve definovaných funkcí `build_generator_signal` a `build_discriminator_signal`. Předávají se jim odpovídající parametry (`LATENT_DIM` pro generátor, `WINDOW_SIZE` pro oba).\n",
        "\n",
        "## Trénovací Krok (`train_step_gan_signal`)\n",
        "*   `tf.function`: Tato dekorace optimalizuje funkci `train_step_gan_signal` pro rychlejší spuštění pomocí TensorFlow grafů.\n",
        "* Tato funkce provádí **jeden trénovací krok** pro GAN (jak pro generátor, tak pro diskriminátor) na jedné dávce dat.\n",
        "* `nois = tf.random.normal(...)`: Generuje **náhodný šum** pro vstup generátoru. Velikost šumu odpovíá počtu signálů v aktuální dávce (`tf.shape(signal_widnows_batch) [0]`) a `LATENT_DIM`.\n",
        "* `with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:` - Toto vytváří **Gradient Tape**. TensorFlow používá Gradient Tapes k zaznamenávání operací, které se mají provést, aby se později mohly automaticky vypočítat gradienty(derivace), které jsou nezbytné pro aktualizaci vah modelu během tréninku. `gen_tape` zaznamenává operace pro generátor a `disc_tape` pro diskriminátor.\n",
        "* `generated_signals = generator_signal_model(noise, training=True)`: Generátor přijímá náhodný šum a vytváří **falešné signály**. `training=True` zajišťuje, že se během tohoto kroku správně chovají vrstvy jako Batch Normalization a Dropout.\n",
        "* `real_output = discriminator_signal_model(signal_widnows_batch, training=True)`: Diskriminátor přijímá **reálné signály** z aktuální dávky trénovacích dat a předpovídá, jak \"reálné\" je považuje.\n",
        "* `fake_output = discriminator_signal_model(generated_signals, training=True)`: Diskriminátor přijímá **falešné signály** vygenerované generátorem a předpovídá, jak \"reálné\" je považuje.\n",
        "* `gen_loss = generator_loss_signal(fake_output)`: Vypočítá **ztrátu generátoru** na základě výstupu diskriminátoru pro falešné signály.\n",
        "* `disc_loss = discriminator_loss_signal(real_output, fake_output)`: Vypočítá **ztrátu diskriminátoru** na základě výstupu pro reálné i falešné signály.\n",
        "* `gradients_of_generator = gen_tape.gradient(gen_loss, generator_signal_model.trainable_varialbes)`: Vypočítá **gradienty ztráty generátoru** vzhledem k trénovatelným proměnným (vahám) generátoru.\n",
        "* `gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator_signal_model.trainable_variables)`: Vypočítá **gradienty ztráty diskriminátoru** vzhledem k trénovatelným proměnným diskriminátoru.\n",
        "* `generator_optimizer_signal.apply_gradients(...)` a `discriminator_optimizer_signal.apply_gradients(...)` Tyto řádky **aktualizují váhy generátoru a diskriminátoru pomocí vypočítaných gradientů a jejich příslušných optimalizátorů. Tím se modely učí minimalizovat své ztráty. Funkce vrací vypočítané ztráty generátoru a diskriminátoru pro tuto dávku.\n",
        "\n",
        "## Hlavní Trénovací Smyčka (`train_gan_signal`)\n",
        "\n",
        "* Tato funkce řídí celý proces trénovaní GANu pro zadaný počet epoch\n",
        "* Jako vstup bere `dataset`(připravený `tf.data.Dataset` s trénovacími daty) a `epochs_count` (počet trénovacích epoch)\n",
        "* Inicialzují se prázdné seznamy pro ukládání historie ztrát generátoru a diskriminátoru v průběhu epoch.\n",
        "* Hlavní smyčka iteruje přes zadaný počet epoch.\n",
        "* Uvnitř smyčky pro každou epochu se inicializují instace tf.keras.metrics.Mean() pro sledování **průměrné ztráty generátoru a diskriminátoru** za aktuální epochu.\n",
        "* Vnitřní smyčka iteruje přes **dávky dat** v `datasetu`.\n",
        "* Pro každou dávku dat se zavolá funkce `train_step_gan_signal` k provedení jednoho trénovacího kroku a získání ztrát pro tuto dávku.\n",
        "* Ztráty z aktuální dávky se aktualizují v metríkách průměrných za epochu.\n",
        "* Po zpracování všech dávek v epoše se získá **finální průměrná ztráta** pro generátor a diskrimátor za tuto epochu.\n",
        "* Průměrné ztráty za epochu se přidájí do seznamů historie ztrát\n",
        "* Každých 10 (nebo v poslední epoše) se **uloží model generátoru**. To umožnuje obnovit trénink nebo použít generátor z konkrétní fáze tréninku. Model se ukládá ve formátu `.keras`\n",
        "* Po dokončení všech epoch funkce vrací seznamy historie ztrát generátoru a diskriminátoru.\n",
        "\n",
        "## Spuštění Tréninku\n",
        "* Volá se funkce `train_gan_signal`, která spouští trénovací smyšku s připraveným datasetem (train_dataset_signal) a zadaným počtem epoch (EPOCHS). Výsledné historie ztrát se ukládají do proměnných `gen_loss_history_signal` a `disc_loss_history_signal`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lUBVUzMb-BUy",
        "outputId": "5d5d74bb-6f46-406e-e0b2-d988a2bc5585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Používám Label Smoothing pro reálné štítky: 0.9\n",
            "Rychlost učení Generátoru: 0.0002, Diskriminátoru: 0.0001\n",
            "--- Architektura Generátoru ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Generator_Signal\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"Generator_Signal\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8064</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">814,464</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_transpose                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1DTranspose</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_transpose_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">252</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1DTranspose</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">252</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">252</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_transpose_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">504</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,272</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1DTranspose</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">504</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">504</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_transpose_3              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">504</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1DTranspose</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ cropping1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cropping1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8064\u001b[0m)           │       \u001b[38;5;34m814,464\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_transpose                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m82,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv1DTranspose\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_transpose_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m252\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m41,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv1DTranspose\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m252\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m252\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_transpose_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m504\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m10,272\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv1DTranspose\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m504\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m504\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_transpose_3              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m504\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │           \u001b[38;5;34m161\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv1DTranspose\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ cropping1d (\u001b[38;5;33mCropping1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">949,377</span> (3.62 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m949,377\u001b[0m (3.62 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">948,673</span> (3.62 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m948,673\u001b[0m (3.62 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> (2.75 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m704\u001b[0m (2.75 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Architektura Diskriminátoru ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Discriminator_Signal\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"Discriminator_Signal\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">164,096</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16128</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │           \u001b[38;5;34m384\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_4 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m41,088\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_5 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m164,096\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_6 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16128\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m16,129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">223,233</span> (872.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m223,233\u001b[0m (872.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">222,465</span> (869.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m222,465\u001b[0m (869.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> (3.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m768\u001b[0m (3.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Spouštím trénování GANu pro signály arteriálního tlaku...\n",
            "Vstupuji do funkce train_gan_signal, počet epoch: 100\n",
            "Začátek epochy 1/100.\n",
            "   Zpracovávám první dávku ((64, 500, 1)) v první epoše...\n",
            "      Volám train_step_gan_signal poprvé...\n",
            "      První volání train_step_gan_signal dokončeno. G_loss: 0.8994, D_loss: 1.7559\n",
            "Epocha 1/100 dokončena. Ztráta G: 3.7225, Ztráta D: 0.6177\n",
            "Začátek epochy 2/100.\n",
            "Epocha 2/100 dokončena. Ztráta G: 5.5056, Ztráta D: 0.4115\n",
            "Začátek epochy 3/100.\n",
            "Epocha 3/100 dokončena. Ztráta G: 5.2955, Ztráta D: 0.4140\n",
            "Začátek epochy 4/100.\n",
            "Epocha 4/100 dokončena. Ztráta G: 4.5746, Ztráta D: 0.5074\n",
            "Začátek epochy 5/100.\n",
            "Epocha 5/100 dokončena. Ztráta G: 5.2065, Ztráta D: 0.4707\n",
            "Začátek epochy 6/100.\n",
            "Epocha 6/100 dokončena. Ztráta G: 5.9587, Ztráta D: 0.3727\n",
            "Začátek epochy 7/100.\n",
            "Epocha 7/100 dokončena. Ztráta G: 5.3986, Ztráta D: 0.4314\n",
            "Začátek epochy 8/100.\n",
            "Epocha 8/100 dokončena. Ztráta G: 4.7288, Ztráta D: 0.5356\n",
            "Začátek epochy 9/100.\n",
            "Epocha 9/100 dokončena. Ztráta G: 4.2684, Ztráta D: 0.6104\n",
            "Začátek epochy 10/100.\n",
            "Epocha 10/100 dokončena. Ztráta G: 4.5697, Ztráta D: 0.5912\n",
            "Začátek epochy 11/100.\n",
            "Epocha 11/100 dokončena. Ztráta G: 4.8603, Ztráta D: 0.5529\n",
            "Začátek epochy 12/100.\n",
            "Epocha 12/100 dokončena. Ztráta G: 4.9798, Ztráta D: 0.5355\n",
            "Začátek epochy 13/100.\n",
            "Epocha 13/100 dokončena. Ztráta G: 5.0713, Ztráta D: 0.5294\n",
            "Začátek epochy 14/100.\n",
            "Epocha 14/100 dokončena. Ztráta G: 4.3798, Ztráta D: 0.5733\n",
            "Začátek epochy 15/100.\n",
            "Epocha 15/100 dokončena. Ztráta G: 4.4908, Ztráta D: 0.5702\n",
            "Začátek epochy 16/100.\n",
            "Epocha 16/100 dokončena. Ztráta G: 4.6518, Ztráta D: 0.5481\n",
            "Začátek epochy 17/100.\n",
            "Epocha 17/100 dokončena. Ztráta G: 4.4137, Ztráta D: 0.5699\n",
            "Začátek epochy 18/100.\n",
            "Epocha 18/100 dokončena. Ztráta G: 4.4438, Ztráta D: 0.5766\n",
            "Začátek epochy 19/100.\n",
            "Epocha 19/100 dokončena. Ztráta G: 4.5244, Ztráta D: 0.5665\n",
            "Začátek epochy 20/100.\n",
            "Epocha 20/100 dokončena. Ztráta G: 4.3529, Ztráta D: 0.5834\n",
            "Začátek epochy 21/100.\n",
            "Epocha 21/100 dokončena. Ztráta G: 4.2816, Ztráta D: 0.5901\n",
            "Začátek epochy 22/100.\n",
            "Epocha 22/100 dokončena. Ztráta G: 4.5038, Ztráta D: 0.5638\n",
            "Začátek epochy 23/100.\n",
            "Epocha 23/100 dokončena. Ztráta G: 4.0140, Ztráta D: 0.6279\n",
            "Začátek epochy 24/100.\n",
            "Epocha 24/100 dokončena. Ztráta G: 3.7996, Ztráta D: 0.6488\n",
            "Začátek epochy 25/100.\n",
            "Epocha 25/100 dokončena. Ztráta G: 3.8899, Ztráta D: 0.6498\n",
            "Začátek epochy 26/100.\n",
            "Epocha 26/100 dokončena. Ztráta G: 3.5646, Ztráta D: 0.6928\n",
            "Začátek epochy 27/100.\n",
            "Epocha 27/100 dokončena. Ztráta G: 3.3471, Ztráta D: 0.7277\n",
            "Začátek epochy 28/100.\n",
            "Epocha 28/100 dokončena. Ztráta G: 3.5201, Ztráta D: 0.7072\n",
            "Začátek epochy 29/100.\n",
            "Epocha 29/100 dokončena. Ztráta G: 3.1954, Ztráta D: 0.7289\n",
            "Začátek epochy 30/100.\n",
            "Epocha 30/100 dokončena. Ztráta G: 2.8304, Ztráta D: 0.7980\n",
            "Začátek epochy 31/100.\n",
            "Epocha 31/100 dokončena. Ztráta G: 2.9757, Ztráta D: 0.7703\n",
            "Začátek epochy 32/100.\n",
            "Epocha 32/100 dokončena. Ztráta G: 2.7269, Ztráta D: 0.8061\n",
            "Začátek epochy 33/100.\n",
            "Epocha 33/100 dokončena. Ztráta G: 2.8169, Ztráta D: 0.7828\n",
            "Začátek epochy 34/100.\n",
            "Epocha 34/100 dokončena. Ztráta G: 2.4893, Ztráta D: 0.8576\n",
            "Začátek epochy 35/100.\n",
            "Epocha 35/100 dokončena. Ztráta G: 2.3910, Ztráta D: 0.8564\n",
            "Začátek epochy 36/100.\n",
            "Epocha 36/100 dokončena. Ztráta G: 2.2843, Ztráta D: 0.8789\n",
            "Začátek epochy 37/100.\n",
            "Epocha 37/100 dokončena. Ztráta G: 2.3423, Ztráta D: 0.8497\n",
            "Začátek epochy 38/100.\n",
            "Epocha 38/100 dokončena. Ztráta G: 2.3082, Ztráta D: 0.8639\n",
            "Začátek epochy 39/100.\n",
            "Epocha 39/100 dokončena. Ztráta G: 2.2963, Ztráta D: 0.8603\n",
            "Začátek epochy 40/100.\n",
            "Epocha 40/100 dokončena. Ztráta G: 2.2370, Ztráta D: 0.8731\n",
            "Začátek epochy 41/100.\n",
            "Epocha 41/100 dokončena. Ztráta G: 2.2808, Ztráta D: 0.8512\n",
            "Začátek epochy 42/100.\n",
            "Epocha 42/100 dokončena. Ztráta G: 2.1799, Ztráta D: 0.8697\n",
            "Začátek epochy 43/100.\n",
            "Epocha 43/100 dokončena. Ztráta G: 2.1605, Ztráta D: 0.8696\n",
            "Začátek epochy 44/100.\n",
            "Epocha 44/100 dokončena. Ztráta G: 2.1351, Ztráta D: 0.8681\n",
            "Začátek epochy 45/100.\n",
            "Epocha 45/100 dokončena. Ztráta G: 2.1321, Ztráta D: 0.8675\n",
            "Začátek epochy 46/100.\n",
            "Epocha 46/100 dokončena. Ztráta G: 2.0781, Ztráta D: 0.8743\n",
            "Začátek epochy 47/100.\n",
            "Epocha 47/100 dokončena. Ztráta G: 2.0758, Ztráta D: 0.8755\n",
            "Začátek epochy 48/100.\n",
            "Epocha 48/100 dokončena. Ztráta G: 2.0555, Ztráta D: 0.8812\n",
            "Začátek epochy 49/100.\n",
            "Epocha 49/100 dokončena. Ztráta G: 2.0445, Ztráta D: 0.8848\n",
            "Začátek epochy 50/100.\n",
            "Epocha 50/100 dokončena. Ztráta G: 2.0155, Ztráta D: 0.8917\n",
            "Začátek epochy 51/100.\n",
            "Epocha 51/100 dokončena. Ztráta G: 2.0471, Ztráta D: 0.8840\n",
            "Začátek epochy 52/100.\n",
            "Epocha 52/100 dokončena. Ztráta G: 1.9732, Ztráta D: 0.9030\n",
            "Začátek epochy 53/100.\n",
            "Epocha 53/100 dokončena. Ztráta G: 1.9774, Ztráta D: 0.8984\n",
            "Začátek epochy 54/100.\n",
            "Epocha 54/100 dokončena. Ztráta G: 1.9891, Ztráta D: 0.8944\n",
            "Začátek epochy 55/100.\n",
            "Epocha 55/100 dokončena. Ztráta G: 2.4596, Ztráta D: 0.8214\n",
            "Začátek epochy 56/100.\n",
            "Epocha 56/100 dokončena. Ztráta G: 4.1066, Ztráta D: 0.4926\n",
            "Začátek epochy 57/100.\n",
            "Epocha 57/100 dokončena. Ztráta G: 3.1931, Ztráta D: 0.5720\n",
            "Začátek epochy 58/100.\n",
            "Epocha 58/100 dokončena. Ztráta G: 3.0408, Ztráta D: 0.6116\n",
            "Začátek epochy 59/100.\n",
            "Epocha 59/100 dokončena. Ztráta G: 2.9343, Ztráta D: 0.6394\n",
            "Začátek epochy 60/100.\n",
            "Epocha 60/100 dokončena. Ztráta G: 2.8632, Ztráta D: 0.6600\n",
            "Začátek epochy 61/100.\n",
            "Epocha 61/100 dokončena. Ztráta G: 2.7799, Ztráta D: 0.6845\n",
            "Začátek epochy 62/100.\n",
            "Epocha 62/100 dokončena. Ztráta G: 2.7143, Ztráta D: 0.7064\n",
            "Začátek epochy 63/100.\n",
            "Epocha 63/100 dokončena. Ztráta G: 2.5840, Ztráta D: 0.7426\n",
            "Začátek epochy 64/100.\n",
            "Epocha 64/100 dokončena. Ztráta G: 2.4629, Ztráta D: 0.7757\n",
            "Začátek epochy 65/100.\n",
            "Epocha 65/100 dokončena. Ztráta G: 2.3881, Ztráta D: 0.8028\n",
            "Začátek epochy 66/100.\n",
            "Epocha 66/100 dokončena. Ztráta G: 2.2972, Ztráta D: 0.8259\n",
            "Začátek epochy 67/100.\n",
            "Epocha 67/100 dokončena. Ztráta G: 2.2268, Ztráta D: 0.8466\n",
            "Začátek epochy 68/100.\n",
            "Epocha 68/100 dokončena. Ztráta G: 2.1827, Ztráta D: 0.8589\n",
            "Začátek epochy 69/100.\n",
            "Epocha 69/100 dokončena. Ztráta G: 2.1429, Ztráta D: 0.8680\n",
            "Začátek epochy 70/100.\n",
            "Epocha 70/100 dokončena. Ztráta G: 2.0904, Ztráta D: 0.8853\n",
            "Začátek epochy 71/100.\n",
            "Epocha 71/100 dokončena. Ztráta G: 2.0366, Ztráta D: 0.9022\n",
            "Začátek epochy 72/100.\n",
            "Epocha 72/100 dokončena. Ztráta G: 2.0289, Ztráta D: 0.9031\n",
            "Začátek epochy 73/100.\n",
            "Epocha 73/100 dokončena. Ztráta G: 1.9920, Ztráta D: 0.9152\n",
            "Začátek epochy 74/100.\n",
            "Epocha 74/100 dokončena. Ztráta G: 1.9744, Ztráta D: 0.9183\n",
            "Začátek epochy 75/100.\n",
            "Epocha 75/100 dokončena. Ztráta G: 1.9797, Ztráta D: 0.9161\n",
            "Začátek epochy 76/100.\n",
            "Epocha 76/100 dokončena. Ztráta G: 1.9134, Ztráta D: 0.9382\n",
            "Začátek epochy 77/100.\n",
            "Epocha 77/100 dokončena. Ztráta G: 1.9335, Ztráta D: 0.9256\n",
            "Začátek epochy 78/100.\n",
            "Epocha 78/100 dokončena. Ztráta G: 1.9098, Ztráta D: 0.9380\n",
            "Začátek epochy 79/100.\n",
            "Epocha 79/100 dokončena. Ztráta G: 1.8898, Ztráta D: 0.9402\n",
            "Začátek epochy 80/100.\n",
            "Epocha 80/100 dokončena. Ztráta G: 1.9184, Ztráta D: 0.9283\n",
            "Začátek epochy 81/100.\n",
            "Epocha 81/100 dokončena. Ztráta G: 1.8934, Ztráta D: 0.9366\n",
            "Začátek epochy 82/100.\n",
            "Epocha 82/100 dokončena. Ztráta G: 1.8785, Ztráta D: 0.9421\n",
            "Začátek epochy 83/100.\n",
            "Epocha 83/100 dokončena. Ztráta G: 1.8665, Ztráta D: 0.9461\n",
            "Začátek epochy 84/100.\n",
            "Epocha 84/100 dokončena. Ztráta G: 1.8497, Ztráta D: 0.9525\n",
            "Začátek epochy 85/100.\n",
            "Epocha 85/100 dokončena. Ztráta G: 1.8438, Ztráta D: 0.9545\n",
            "Začátek epochy 86/100.\n",
            "Epocha 86/100 dokončena. Ztráta G: 1.8359, Ztráta D: 0.9589\n",
            "Začátek epochy 87/100.\n",
            "Epocha 87/100 dokončena. Ztráta G: 1.8248, Ztráta D: 0.9628\n",
            "Začátek epochy 88/100.\n",
            "Epocha 88/100 dokončena. Ztráta G: 1.8023, Ztráta D: 0.9681\n",
            "Začátek epochy 89/100.\n",
            "Epocha 89/100 dokončena. Ztráta G: 1.8048, Ztráta D: 0.9651\n",
            "Začátek epochy 90/100.\n",
            "Epocha 90/100 dokončena. Ztráta G: 1.7849, Ztráta D: 0.9747\n",
            "Začátek epochy 91/100.\n",
            "Epocha 91/100 dokončena. Ztráta G: 1.7772, Ztráta D: 0.9781\n",
            "Začátek epochy 92/100.\n",
            "Epocha 92/100 dokončena. Ztráta G: 1.7578, Ztráta D: 0.9825\n",
            "Začátek epochy 93/100.\n",
            "Epocha 93/100 dokončena. Ztráta G: 1.7539, Ztráta D: 0.9846\n",
            "Začátek epochy 94/100.\n",
            "Epocha 94/100 dokončena. Ztráta G: 1.7499, Ztráta D: 0.9840\n",
            "Začátek epochy 95/100.\n",
            "Epocha 95/100 dokončena. Ztráta G: 1.7444, Ztráta D: 0.9904\n",
            "Začátek epochy 96/100.\n",
            "Epocha 96/100 dokončena. Ztráta G: 1.7219, Ztráta D: 0.9938\n",
            "Začátek epochy 97/100.\n",
            "Epocha 97/100 dokončena. Ztráta G: 1.7173, Ztráta D: 0.9977\n",
            "Začátek epochy 98/100.\n",
            "Epocha 98/100 dokončena. Ztráta G: 1.7098, Ztráta D: 0.9996\n",
            "Začátek epochy 99/100.\n",
            "Epocha 99/100 dokončena. Ztráta G: 1.7110, Ztráta D: 0.9960\n",
            "Začátek epochy 100/100.\n",
            "Epocha 100/100 dokončena. Ztráta G: 1.7137, Ztráta D: 0.9970\n",
            "Trénování GANu pro signály dokončeno a modely uloženy.\n"
          ]
        }
      ],
      "source": [
        "# ==== Trénování GANu ====\n",
        "cross_entropy_signal = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "def discriminator_loss_signal(real_output, fake_output):\n",
        "    real_labels = tf.ones_like(real_output) * LABEL_SMOOTHING_REAL\n",
        "    real_loss = cross_entropy_signal(real_labels, real_output)\n",
        "\n",
        "    fake_labels = tf.ones_like(fake_output) * LABEL_SMOOTHING_FAKE\n",
        "    fake_loss = cross_entropy_signal(fake_labels, fake_output)\n",
        "\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss_signal(fake_output):\n",
        "  return cross_entropy_signal(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "print(f\"Používám Label Smoothing pro reálné štítky: {LABEL_SMOOTHING_REAL}\")\n",
        "print(f\"Rychlost učení Generátoru: {LEARNING_RATE_G}, Diskriminátoru: {LEARNING_RATE_D}\")\n",
        "\n",
        "generator_optimizer_signal = optimizers.Adam(LEARNING_RATE_G, beta_1=0.5)\n",
        "discriminator_optimizer_signal = optimizers.Adam(LEARNING_RATE_D, beta_1=0.5)\n",
        "\n",
        "generator_signal_model = build_generator_signal(LATENT_DIM, WINDOW_SIZE)\n",
        "discriminator_signal_model = build_discriminator_signal(WINDOW_SIZE)\n",
        "\n",
        "print(\"--- Architektura Generátoru ---\")\n",
        "generator_signal_model.summary()\n",
        "print(\"\\n--- Architektura Diskriminátoru ---\")\n",
        "discriminator_signal_model.summary()\n",
        "\n",
        "@tf.function\n",
        "def train_step_gan_signal(signal_windows_batch):\n",
        "    noise = tf.random.normal([tf.shape(signal_windows_batch)[0], LATENT_DIM])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_signals = generator_signal_model(noise, training=True)\n",
        "\n",
        "        real_output = discriminator_signal_model(signal_windows_batch, training=True)\n",
        "        fake_output = discriminator_signal_model(generated_signals, training=True)\n",
        "\n",
        "        gen_loss = generator_loss_signal(fake_output)\n",
        "        disc_loss = discriminator_loss_signal(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator_signal_model.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator_signal_model.trainable_variables)\n",
        "\n",
        "    generator_optimizer_signal.apply_gradients(zip(gradients_of_generator, generator_signal_model.trainable_variables))\n",
        "    discriminator_optimizer_signal.apply_gradients(zip(gradients_of_discriminator, discriminator_signal_model.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n",
        "\n",
        "def train_gan_signal(dataset, epochs_count):\n",
        "    print(f\"Vstupuji do funkce train_gan_signal, počet epoch: {epochs_count}\")\n",
        "    history_gen_loss = []\n",
        "    history_disc_loss = []\n",
        "\n",
        "    for epoch in range(epochs_count):\n",
        "        print(f\"Začátek epochy {epoch+1}/{epochs_count}.\")\n",
        "        epoch_gen_loss_avg = tf.keras.metrics.Mean()\n",
        "        epoch_disc_loss_avg = tf.keras.metrics.Mean()\n",
        "\n",
        "        batch_idx = 0\n",
        "        for signal_batch in dataset:\n",
        "            if batch_idx == 0 and epoch == 0:\n",
        "                print(f\"   Zpracovávám první dávku ({signal_batch.shape}) v první epoše...\")\n",
        "                print(\"      Volám train_step_gan_signal poprvé...\")\n",
        "\n",
        "            gen_loss, disc_loss = train_step_gan_signal(signal_batch)\n",
        "\n",
        "            if batch_idx == 0 and epoch == 0:\n",
        "                print(f\"      První volání train_step_gan_signal dokončeno. G_loss: {gen_loss:.4f}, D_loss: {disc_loss:.4f}\")\n",
        "\n",
        "            epoch_gen_loss_avg.update_state(gen_loss)\n",
        "            epoch_disc_loss_avg.update_state(disc_loss)\n",
        "            batch_idx += 1\n",
        "\n",
        "        g_loss_val = epoch_gen_loss_avg.result().numpy()\n",
        "        d_loss_val = epoch_disc_loss_avg.result().numpy()\n",
        "        history_gen_loss.append(g_loss_val)\n",
        "        history_disc_loss.append(d_loss_val)\n",
        "\n",
        "        print(f\"Epocha {epoch+1}/{epochs_count} dokončena. Ztráta G: {g_loss_val:.4f}, Ztráta D: {d_loss_val:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % 10 == 0 or epoch == epochs_count - 1 :\n",
        "            generator_signal_model.save(os.path.join(MODEL_DIR_SIGNAL_GAN, f\"gan_generator_signal_epoch_{epoch+1}.keras\"))\n",
        "\n",
        "    return history_gen_loss, history_disc_loss\n",
        "\n",
        "print(\"\\nSpouštím trénování GANu pro signály arteriálního tlaku...\")\n",
        "gen_loss_history_signal, disc_loss_history_signal = train_gan_signal(train_dataset_signal, EPOCHS)\n",
        "\n",
        "\n",
        "print(\"Trénování GANu pro signály dokončeno a modely uloženy.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8jZCu4UxWNq"
      },
      "source": [
        "# Generace signálu a vizualizace\n",
        "\n",
        "Tato sekce kodu se zaměřuje na generování syntetických signálů použitím natrénovaného GAN generátoru.\n",
        "\n",
        "*   `generate_signals(generator_model, scaler_model, n_signals=, latent_dim=LATENT_DIM)`\n",
        "  * Bere natrénovaný `generator_model`, ke škálování dat se použije `scaler_model`, `n_signals` je požadovaný počet signálu k vygenerování, a `latent_dim`(velikost noise vektoru) jako vstup\n",
        "  * `noise = tf.random.normal([n_signals, latent_dim])`: Tento řádek generuje batch náhodného noise vektoru. Velikost batch je `n_signals` a každý vektor má délku `latent_dim`\n",
        "  *  `generated_scaled_signals = generator_model.predict(noise, verbose=0)`: Vygenerovaný šum je nakrmen do `generator_model`. Ta `predict` metoda je použita protože používáme ten generátor který už je naučený, `verbose=0` potlačuje výstup z toho prediktivního procesu. Výstup `generated_scaled_signals` je batch syntetických signálů která jsou stále v škálovaném rozsahu (-1 až 1)\n",
        "  * Kod pak iteruje skrze všechny `n_signals` vygenerovaných signálů.\n",
        "  * `signal_unscaled_one = scaler_mode.inverse_transform(signal_scaled_one.reshape(-1, 1)).flatten()`: Toto je důležitý krok. Vyškálované signály se musí vrátit do původníhu rozsahu a to pomocí `inverse_transform` metody z `scaler_model`. `reshape(-1,1)` říká jaký tvar má ted. `flatten()` konvertuje výsledný 2D pole zpátky do 1D pole což reprezentuje neškálovaný signál.\n",
        "  * Tato funkce pak vrací seznam neškálovaných syntetických signálů.\n",
        "\n",
        "*   Po definici funkce se snažíme načíst uložený `MinMaxScaler` objekt použitím `joblib.load`. Toto je důležité když chceme použít generační proces v separátním skriptu po trénování.\n",
        "\n",
        "* `generated_art_signals = generate_signals(generator_signal_model, loaded_signal_scaler, n_signals=3)`: Tento řádek volá `generate_signals` funkci k vytvoření 3 syntetických \"art\" signálů s použitím natrénovaného `generator_signal_model` a načteného `loaded_signal_scaler`\n",
        "\n",
        "* Poté jsou vykreslena pomocí `plt` a uložena jako hdf5 soubory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAB0GEET-Fli"
      },
      "outputs": [],
      "source": [
        " # ==== Generování Signálů ====\n",
        "def generate_signals(generator_model, scaler_model, n_signals=5, latent_dim=LATENT_DIM):\n",
        "    noise = tf.random.normal([n_signals, latent_dim])\n",
        "    generated_scaled_signals = generator_model.predict(noise, verbose=0)\n",
        "\n",
        "    generated_signals_unscaled_list = []\n",
        "    for i in range(n_signals):\n",
        "        signal_scaled_one = generated_scaled_signals[i, :, 0]\n",
        "        signal_unscaled_one = scaler_model.inverse_transform(signal_scaled_one.reshape(-1, 1)).flatten()\n",
        "        generated_signals_unscaled_list.append(signal_unscaled_one)\n",
        "\n",
        "    return generated_signals_unscaled_list\n",
        "\n",
        "    try:\n",
        "        loaded_signal_scaler = joblib.load(os.path.join(MODEL_DIR_SIGNAL_GAN, 'signal_scaler.gz'))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Chyba: Soubor se scalerem '{os.path.join(MODEL_DIR_SIGNAL_GAN, 'signal_scaler.gz')}' nebyl nalezen. Používám scaler z trénování.\")\n",
        "        loaded_signal_scaler = signal_scaler\n",
        "\n",
        "    generated_art_signals = generate_signals(generator_signal_model, loaded_signal_scaler, n_signals=3)\n",
        "\n",
        "\n",
        "    # ==== Vizualizace ====\n",
        "    if X_signal_windows_scaled.size > 0 and len(generated_art_signals) > 0:\n",
        "        real_signal_window_scaled_example = next(iter(train_dataset_signal))[0].numpy()\n",
        "        real_signal_window_unscaled_example = loaded_signal_scaler.inverse_transform(real_signal_window_scaled_example[:,0].reshape(-1,1)).flatten()\n",
        "\n",
        "        plt.figure(figsize=(18, 6 * ((len(generated_art_signals) + 1) // 2)))\n",
        "        plt.subplot((len(generated_art_signals) + 1) // 2, 2, 1)\n",
        "        plt.plot(real_signal_window_unscaled_example)\n",
        "        plt.title(f\"Příklad Reálného Signálu 'art' (okno {WINDOW_SIZE} vzorků)\")\n",
        "        plt.xlabel(\"Vzorek\")\n",
        "        plt.ylabel(\"Arteriální tlak\")\n",
        "\n",
        "        for i, gen_signal in enumerate(generated_art_signals):\n",
        "            plt.subplot((len(generated_art_signals) + 1) // 2, 2, i + 2)\n",
        "            plt.plot(gen_signal)\n",
        "            plt.title(f\"Generovaný Signál 'art' {i+1} (okno {WINDOW_SIZE} vzorků)\")\n",
        "            plt.xlabel(\"Vzorek\")\n",
        "            plt.ylabel(\"Arteriální tlak\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    if 'gen_loss_history_signal' in globals() and 'disc_loss_history_signal' in globals():\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(gen_loss_history_signal, label='Ztráta Generátoru (signál)')\n",
        "        plt.plot(disc_loss_history_signal, label='Ztráta Diskriminátoru (signál)')\n",
        "        plt.title(\"Historie Ztrát GANu (signál 'art')\")\n",
        "        plt.xlabel('Epocha')\n",
        "        plt.ylabel('Ztráta')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Historie trénování pro GAN na signálech není k dispozici.\")\n",
        "\n",
        "    output_hdf5_generated_signals_file = os.path.join(MODEL_DIR_SIGNAL_GAN, \"generated_art_signals.hdf5\")\n",
        "    with h5py.File(output_hdf5_generated_signals_file, \"w\") as f_out:\n",
        "        for i, signal_data in enumerate(generated_art_signals):\n",
        "            dataset_name = f\"art_synthetic_{i}\"\n",
        "            f_out.create_dataset(dataset_name, data=signal_data)\n",
        "    print(f\"Vygenerované signály 'art' uloženy do {output_hdf5_generated_signals_file}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
